{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#필수 라이브러리를 호출합니다.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'konlpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-64b127962d56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#파이썬 한국어 자연어처리 라이브러리인 KoNLPy의 Mecab을 불러옵니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMecab\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#KoNLPy에 대한 자세한 내용은 다음 링크를 확인해주세요(https://konlpy-ko.readthedocs.io/ko/v0.4.3/)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#Mecab에 대한 자세한 설명은 다음 링크를 확인해주세요(http://eunjeon.blogspot.com/2013/02/blog-post.html)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#현재 konlpy에서 메캅은 윈도우즈 환경에서 구동할 수 없습니다. linux 또는 Mac OS를 써주세요.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'konlpy'"
     ]
    }
   ],
   "source": [
    "#파이썬 한국어 자연어처리 라이브러리인 KoNLPy의 Mecab을 불러옵니다.\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "#KoNLPy에 대한 자세한 내용은 다음 링크를 확인해주세요(https://konlpy-ko.readthedocs.io/ko/v0.4.3/)\n",
    "#Mecab에 대한 자세한 설명은 다음 링크를 확인해주세요(http://eunjeon.blogspot.com/2013/02/blog-post.html)\n",
    "#현재 konlpy에서 메캅은 윈도우즈 환경에서 구동할 수 없습니다. linux 또는 Mac OS를 써주세요.\n",
    "\n",
    "mecab = Mecab(dicpath='@@@') \n",
    "#메캅의 사용자사전을 불러옵니다. dicpath 이하 @@@은 해당 사전의 경로입니다. 앞으로 모든 @@@은 해당 파일이 있는 경로를 의미합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#분석용 파일을 불러옵니다. 우선은 범여_편집본2.csv를 보겠습니다.\n",
    "#편집본은 현행 사전기반 자연어처리의 한계를 극복하기 위해 텍스트 원본의 일부를 수정한 것입니다.(1차 전처리))\n",
    "#예컨대 기존 사전에 없는 '자유한국당'이라는 단어는 '자유', '한국', '당'으로 처리됩니다. \n",
    "#이를 방지하기 위해 '자유한국당'(과 동의어인 '한국당')은 일괄 '새누리당'으로 변경했습니다.(그럼에도 '누리'만 남아서 실 분석에서는 '누리'를 '자유한국당'으로 변경했습니다.)\n",
    "df7 = pd.read_csv('@@@')\n",
    "print(df7.shape)#총 몇 줄인지 알아봅니다.\n",
    "print(df7.content.str.len().sum())#총 몇자인지 알아봅니다.\n",
    "df7.head()#맨 처음 다섯문장을 보여줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#위와 같이, 지난 필리버스터에서 범여권은 38만여 음절을 말했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#혹시 있을수 있는 NaN value를 없애줍니다.\n",
    "df7 = df7.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#본격적으로 단어를 추출합니다.\n",
    "morphs = df7.content.apply(mecab.nouns)#명사만 추출합니다. \n",
    "morphs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2차 전처리를 진행합니다.\n",
    "vocab = defaultdict(int)\n",
    "for words in morphs:\n",
    "    for word in words:\n",
    "        if word.isnumeric():\n",
    "            vocab['NUMBER'] += 1        # 숫자는 `NUMBER`로 대체하고,\n",
    "        elif re.search(r'^\\W+$', word): \n",
    "            pass                        #물음표 등 기호는 제거합니다.\n",
    "        else:\n",
    "            vocab[word] += 1            #2차 전처리된 단어들을 vocab에 입력합니다.\n",
    "\n",
    "vocab                                   #vocab을 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#저렇게 보긴 어려우니까, 빈도 기준 내림차순을 진행하겠습니다.\n",
    "\n",
    "pd.DataFrame.from_dict(vocab, \n",
    "                       orient='index', \n",
    "                       columns=['빈도']).sort_values('빈도', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#제외어stop words를 지정합니다,\n",
    "stop_words=['우리','여러분','사람','나라',\n",
    "            '의원','국회의원',\n",
    "            '얘기','이것','말씀','그것','다음','때문','이게','하나','겁니다','생각',\n",
    "            '여기','어디','이후','이야기','경우','자신','가지','그때','저희','당시',\n",
    "            '겁니까','이번','거기','누','이상','내년','뭡니까','마찬가지','누구',\n",
    "            '이름','무엇','대','나중','여','당신','뭔지','지난해','일부','얼마','국',\n",
    "            '의','한마디'\n",
    "           ]\n",
    "\n",
    "#동의어thesaurus를 지정합니다. (딕셔너리로 지정)\n",
    "thes = {\n",
    "    '수처':'공수처',\n",
    "    '비례제':'비례대표제',\n",
    "    '비례':'비례대표제',\n",
    "    '비례대표':'비례대표제',\n",
    "    '필리':'필리버스터',\n",
    "    '버스터':'필리버스터',\n",
    "    '여당':'민주당',\n",
    "    '더민주':'민주당',\n",
    "    '국당':'한국당',\n",
    "    '한국당':'자유한국당',\n",
    "    '새누리당':'자유한국당',\n",
    "    '문재':'문재인',\n",
    "    '패스트':'패스트트랙',\n",
    "    '트랙':'패스트트랙',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#위에서 세팅한 유의어, 제외어를 적용합니다.\n",
    "reform = []\n",
    "#제외어 처리\n",
    "for w in vocab:\n",
    "    if w not in stop_words:\n",
    "        reform.append(w)\n",
    "#유의어 처리\n",
    "for idx, word in enumerate(reform):\n",
    "    keys = thes.keys()\n",
    "    if word in keys:\n",
    "        reform[idx] = thes[word]\n",
    "reform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reform 기준으로 문서 morphs 정제\n",
    "morphs = morphs.apply(lambda row: [w for w in row if w in reform])\n",
    "morphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#완성된 morph를 기준으로 단어들을 벡터화 합니다.\n",
    "#벡터화는 컴퓨터가 인간의 언어를 이해하게 하기 위한 필수적인 과정입니다. \n",
    "#자세한 내용은 링크를 확인해주세요(https://lovit.github.io/nlp/2018/03/26/from_text_to_matrix/)\n",
    "count_model = CountVectorizer(max_features=200, stop_words=stop_words) #상위 200단어만 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#앞서 불러온 sklearn의 countvectorizer에 입력하기 위하여, 따로 떼어져 있는 단어들을 다시 붙여줍니다.\n",
    "data = morphs.apply(' '.join)\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#벡터라이징 합니다.\n",
    "vect = count_model.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#의미연결망분석은 결국 단어들 간의 동시출현빈도표를 시각화한 것입니다. \n",
    "#지금까지의 전처리와 벡터화 역시 동시출현빈도표를 만들기 위한 과정이었습니다.\n",
    "\n",
    "mat = (vect.T * vect) #200x200 짜리 매트릭스를 만듭니다.\n",
    "mat.setdiag(0) # 같은 단어의 동시출현빈도는 0으로 일괄 치환합니다.\n",
    "print(mat.todense()) # 매트릭스를 dense포맷으로 가져옵니다\n",
    "#이 코드는 여기서 가져왔습니다 : http://www.pingshiuanchua.com/blog/post/keyword-network-analysis-with-python-and-gephi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
